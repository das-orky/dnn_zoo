{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a6165807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "from torch.hub import load_state_dict_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "519bfc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESNET(nn.Module):\n",
    "    def __init__(self,block,n,arch,num_classes=10,init_weight=True,**kwargs):\n",
    "        super(RESNET,self).__init__()\n",
    "        self.in_channels=64\n",
    "        self.conv1=nn.Conv2d(in_channels=1,out_channels=self.in_channels,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        \n",
    "        if arch=='18' or arch =='34':\n",
    "            shortcuts=[False,True,True,True]\n",
    "        else:\n",
    "            shortcuts=[True,True,True,True]\n",
    "            \n",
    "        self.layer1=self.make_layer(block,64,n[0],shortcuts[0],1)\n",
    "        self.layer2=self.make_layer(block,128,n[1],shortcuts[1],2)\n",
    "        self.layer3=self.make_layer(block,256,n[2],shortcuts[2],2)\n",
    "        self.layer4=self.make_layer(block,512,n[2],shortcuts[3],2)\n",
    "        \n",
    "        self.avgpool=nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc=nn.Linear(512*block.expansion,num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x=self.relu(self.bn1(self.conv1(x)))\n",
    "        x=self.maxpool(x)\n",
    "        \n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        x=self.layer4(x)\n",
    "        \n",
    "        x=self.avgpool(x)\n",
    "        x=x.view(x.size(0), -1)\n",
    "        x=self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def make_layer(self,block,out_channels,sub_layers,sampling,strides):\n",
    "        layer=[]\n",
    "        for i in range(sub_layers):\n",
    "            if sampling==True and i==0:\n",
    "                layer.append(block(self.in_channels,out_channels,True,stride=strides))\n",
    "                self.in_channels=out_channels*block.expansion\n",
    "            elif sampling==True and i>0:\n",
    "                layer.append(block(self.in_channels,out_channels,False,stride=1))\n",
    "                self.in_channels=out_channels*block.expansion\n",
    "            elif sampling==False:\n",
    "                layer.append(block(self.in_channels,out_channels,False))\n",
    "                self.in_channels=out_channels*block.expansion\n",
    "        return nn.Sequential(*layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "93495202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(pretrain=False,progress=True,**kwargs):\n",
    "    return _resnet(BasicBlock,[2,2,2,2],'18',pretrain,progress,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "017c8725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resnet(block,n,arch,pretrain,progress,**kwargs):\n",
    "    if pretrain:\n",
    "        kwargs['init_weight']=False\n",
    "    model=RESNET(block,n,arch,**kwargs)\n",
    "    \n",
    "    if pretrain:\n",
    "        state_dict=load_state_dict_from_url(model_urls[arch],progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "02292ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion=1\n",
    "    def __init__(self,in_channels,out_channels,sampling=False,stride=1):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,stride=stride,padding=1,bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(out_channels)\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        self.conv2=nn.Conv2d(in_channels=out_channels,out_channels=out_channels,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bn2=nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.sampling=sampling\n",
    "        if self.sampling:\n",
    "            self.downsample=nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=1,stride=2,padding=0,bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    def forward(self,x):\n",
    "        identity=x.clone()\n",
    "        out=self.conv1(x)\n",
    "        out=self.bn1(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.conv2(out)\n",
    "        out=self.bn2(out)\n",
    "        \n",
    "        if self.sampling:\n",
    "            identity=self.downsample(identity)\n",
    "            \n",
    "        out+=identity\n",
    "        out=self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "17c8b32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0cc3a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_config=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()    \n",
    "])\n",
    "#transforms.Lambda(lambda x: x.repeat(3, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "44cd19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=datasets.FashionMNIST('/home/ubuntu/gpu_work',download=True,train=True,transform=transform_config)\n",
    "test_dataset=datasets.FashionMNIST('/home/ubuntu/gpu_work',download=True,train=False,transform=transform_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6efd9dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=256\n",
    "train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_loader=torch.utils.data.DataLoader(test_dataset,batch_size=256,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7e5dc1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=resnet18().to(device)\n",
    "optimizer=optim.Adam(params=model.parameters(),lr=0.0001)\n",
    "loss_fn=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c6f38c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,optimizer,device,epoch):\n",
    "    model.train()\n",
    "    for batch_ids, (data,label) in enumerate(train_loader):\n",
    "        label=label.type(torch.LongTensor)\n",
    "        data,label=data.to(device),label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model_output=model(data)\n",
    "        loss=loss_fn(model_output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_ids+1)%50 == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch,batch_ids*len(data),len(train_loader.dataset),\n",
    "                100.*batch_ids/len(train_loader),loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a42b7563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_loader,device):\n",
    "    model.eval()\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for data,label in test_loader:\n",
    "            data,label=data.to(device),label.to(device)\n",
    "            y_hat=model(data)\n",
    "            _,y_pred=torch.max(y_hat,1)\n",
    "            correct+=(y_pred==label).sum().item()\n",
    "        print(\"\\n Test Set: Average loss: xx , Accuracy:{}/{} ({:.0f}%)\".format(\n",
    "            correct,len(test_dataset),100.*correct/len(test_dataset)))\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ab2d5e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [12544/60000 (21%)]\tLoss: 0.239765\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 0.204617\n",
      "Train Epoch: 1 [38144/60000 (63%)]\tLoss: 0.265138\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 0.226589\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:8960/10000 (90%)\n",
      "==================================================\n",
      "Train Epoch: 2 [12544/60000 (21%)]\tLoss: 0.152474\n",
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 0.184635\n",
      "Train Epoch: 2 [38144/60000 (63%)]\tLoss: 0.226043\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 0.117694\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:9075/10000 (91%)\n",
      "==================================================\n",
      "Train Epoch: 3 [12544/60000 (21%)]\tLoss: 0.170724\n",
      "Train Epoch: 3 [25344/60000 (42%)]\tLoss: 0.124933\n",
      "Train Epoch: 3 [38144/60000 (63%)]\tLoss: 0.161017\n",
      "Train Epoch: 3 [50944/60000 (85%)]\tLoss: 0.107335\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:9180/10000 (92%)\n",
      "==================================================\n",
      "Train Epoch: 4 [12544/60000 (21%)]\tLoss: 0.118939\n",
      "Train Epoch: 4 [25344/60000 (42%)]\tLoss: 0.107806\n",
      "Train Epoch: 4 [38144/60000 (63%)]\tLoss: 0.111666\n",
      "Train Epoch: 4 [50944/60000 (85%)]\tLoss: 0.079644\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:8911/10000 (89%)\n",
      "==================================================\n",
      "Train Epoch: 5 [12544/60000 (21%)]\tLoss: 0.070200\n",
      "Train Epoch: 5 [25344/60000 (42%)]\tLoss: 0.057058\n",
      "Train Epoch: 5 [38144/60000 (63%)]\tLoss: 0.058192\n",
      "Train Epoch: 5 [50944/60000 (85%)]\tLoss: 0.111576\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:9031/10000 (90%)\n",
      "==================================================\n",
      "Train Epoch: 6 [12544/60000 (21%)]\tLoss: 0.032349\n",
      "Train Epoch: 6 [25344/60000 (42%)]\tLoss: 0.065836\n",
      "Train Epoch: 6 [38144/60000 (63%)]\tLoss: 0.070378\n",
      "Train Epoch: 6 [50944/60000 (85%)]\tLoss: 0.085034\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:9095/10000 (91%)\n",
      "==================================================\n",
      "Train Epoch: 7 [12544/60000 (21%)]\tLoss: 0.034975\n",
      "Train Epoch: 7 [25344/60000 (42%)]\tLoss: 0.052933\n",
      "Train Epoch: 7 [38144/60000 (63%)]\tLoss: 0.063798\n",
      "Train Epoch: 7 [50944/60000 (85%)]\tLoss: 0.058816\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:8799/10000 (88%)\n",
      "==================================================\n",
      "Train Epoch: 8 [12544/60000 (21%)]\tLoss: 0.041209\n",
      "Train Epoch: 8 [50944/60000 (85%)]\tLoss: 0.029800\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:9081/10000 (91%)\n",
      "==================================================\n",
      "Train Epoch: 9 [12544/60000 (21%)]\tLoss: 0.024526\n",
      "Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.011204\n",
      "Train Epoch: 9 [38144/60000 (63%)]\tLoss: 0.012340\n",
      "Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.038780\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:9099/10000 (91%)\n",
      "==================================================\n",
      "Train Epoch: 10 [12544/60000 (21%)]\tLoss: 0.008664\n",
      "Train Epoch: 10 [25344/60000 (42%)]\tLoss: 0.013443\n",
      "Train Epoch: 10 [38144/60000 (63%)]\tLoss: 0.014372\n",
      "Train Epoch: 10 [50944/60000 (85%)]\tLoss: 0.033857\n",
      "\n",
      " Test Set: Average loss: xx , Accuracy:9113/10000 (91%)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    seed=42\n",
    "    EPOCHS=10\n",
    "    \n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        train(model,train_loader,optimizer,device,epoch)\n",
    "        test(model,test_loader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ffabfccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "#del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229af94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearn] *",
   "language": "python",
   "name": "conda-env-deeplearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
